{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":7254401,"sourceType":"datasetVersion","datasetId":2820075},{"sourceId":7264407,"sourceType":"datasetVersion","datasetId":3954249,"isSourceIdPinned":true}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install sparse_dot_topn for fast sparse vector matching\n\nhttps://github.com/ing-bank/sparse_dot_topn","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/sparse-dot-topn-033/sparse_dot_topn-0.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2024-01-08T06:15:07.556378Z","iopub.execute_input":"2024-01-08T06:15:07.557174Z","iopub.status.idle":"2024-01-08T06:15:44.343250Z","shell.execute_reply.started":"2024-01-08T06:15:07.557139Z","shell.execute_reply":"2024-01-08T06:15:44.341782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define TextMatcher","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sparse_dot_topn import awesome_cossim_topn\n\n\nclass TextMatcher:\n    def __init__(self, ground_truth, col, topk=5, lower_bound=-1):\n        self.ground_truth = ground_truth\n        self.vec = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"word\", token_pattern=r\"(?u)(\\b\\w\\w+\\b|[\\.,!])\",\n                                   use_idf=False, min_df=2, binary=True)\n        self.topk = topk\n        self.lower_bound = lower_bound\n        self.col = col\n        \n    def get_matches_df(self, sparse_matrix, texts):\n        non_zeros = sparse_matrix.nonzero()\n\n        text_indices = non_zeros[0]\n        gt_indices = non_zeros[1]\n\n        left_side = np.empty(gt_indices.size, dtype=object)\n        right_side = np.empty(gt_indices.size, dtype=object)\n        match_score = np.zeros(gt_indices.size)\n\n        for index in range(gt_indices.size):\n            left_side[index] = texts.values[text_indices[index]]\n            right_side[index] = self.ground_truth[self.col].values[gt_indices[index]]\n            match_score[index] = sparse_matrix.data[index]\n\n        res_df = pd.DataFrame({self.col: left_side,\n                               'Ground Truth': right_side,\n                               'match_score': match_score})\n\n        res_df = pd.DataFrame(texts).merge(res_df, on=self.col, how=\"left\")\n        return res_df\n\n\n    def match(self, texts_to_match, n_threads=16):\n        print(f\"Matching {texts_to_match.shape[0]} texts to {self.ground_truth.shape[0]} texts...\")\n        \n        X = self.vec.fit_transform(texts_to_match[self.col])\n        X_gt = self.vec.transform(self.ground_truth[self.col])\n        \n        sparse_sim = awesome_cossim_topn(X, X_gt.T, self.topk, self.lower_bound, use_threads=True, n_jobs=n_threads)\n        \n        return self.get_matches_df(sparse_sim, texts_to_match[self.col])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T05:45:22.636005Z","iopub.execute_input":"2024-01-11T05:45:22.636510Z","iopub.status.idle":"2024-01-11T05:45:23.867292Z","shell.execute_reply.started":"2024-01-11T05:45:22.636469Z","shell.execute_reply":"2024-01-11T05:45:23.865671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T07:38:41.410388Z","iopub.execute_input":"2024-01-11T07:38:41.410973Z","iopub.status.idle":"2024-01-11T07:38:41.434254Z","shell.execute_reply.started":"2024-01-11T07:38:41.410933Z","shell.execute_reply":"2024-01-11T07:38:41.433188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get kth similarity score\n\n30th best match score within likely students and within likely LLMs are calculated.","metadata":{}},{"cell_type":"code","source":"TOPK = 30\n\n\ndef get_match_score(df, gt_filter_col):\n    tm = TextMatcher(df[df[gt_filter_col]].reset_index(drop=True), \"text\", topk=TOPK)\n    res_df = tm.match(df, n_threads=4)\n    df = res_df.groupby(\"text\")[\"match_score\"].min().reset_index().merge(df, on=\"text\")\n    return df\n\ndef find_matchscore(df):\n    all_prompts = df[\"prompt_id\"].unique()\n\n    sub_dfs = [get_match_score(df[df[\"prompt_id\"] == pid], \"likely_student\").reset_index(drop=True)[[\"id\", \"match_score\"]]\n               for pid in all_prompts]\n    sub_df = pd.concat(sub_dfs).rename(columns={\"match_score\": \"match_score_student\"})\n\n\n    sub_dfs = [get_match_score(df[df[\"prompt_id\"] == pid], \"likely_llm\").reset_index(drop=True)[[\"id\", \"match_score\"]]\n               for pid in all_prompts]\n    sub_df2 = pd.concat(sub_dfs).rename(columns={\"match_score\": \"match_score_llm\"})\n\n    sub_df = sub_df.merge(sub_df2, on=\"id\")\n    return sub_df","metadata":{"execution":{"iopub.status.busy":"2024-01-08T06:19:49.768425Z","iopub.execute_input":"2024-01-08T06:19:49.768843Z","iopub.status.idle":"2024-01-08T06:19:57.688660Z","shell.execute_reply.started":"2024-01-08T06:19:49.768808Z","shell.execute_reply":"2024-01-08T06:19:57.687228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submission\n\nRatio between student match score and smoothed LLM match score determines the ranking of essays.","metadata":{}},{"cell_type":"code","source":"import sys\nimport gc\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nimport os\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom transformers import AutoTokenizer\nif len(test.text.values) <= 5:\n    sub.to_csv('submission.csv', index=False)\nelse:\n    model_checkpoint = \"/kaggle/input/detect-llm-models/distilroberta-finetuned_v5/checkpoint-9028\"\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    def preprocess_function(examples):\n        return tokenizer(examples['text'], max_length = 512 , padding=True, truncation=True)\n    num_labels = 2\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        # Move your model and data to the GPU\n    model.to(device);\n    trainer = Trainer(\n        model,\n        tokenizer=tokenizer,\n    )\n    test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n    test_ds = Dataset.from_pandas(test)\n    test_ds_enc = test_ds.map(preprocess_function, batched=True)\n    test_preds = trainer.predict(test_ds_enc)\n    logits = test_preds.predictions\n    probs = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:,0]\n    test[\"probs\"]=probs\n    test[\"likely_student\"] = test[\"probs\"] < 0.1\n    test[\"likely_llm\"] = test[\"probs\"] > 0.9\n    sub=find_matchscore(test)\n    SMOOTH = 0.15\n    sub[\"generated\"] = -sub[\"match_score_student\"] / (sub[\"match_score_llm\"] + SMOOTH)\n    sub.to_csv(\"submission.csv\", index=False, columns=[\"id\", \"generated\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T06:01:39.702755Z","iopub.execute_input":"2024-01-11T06:01:39.703279Z","iopub.status.idle":"2024-01-11T06:13:52.694765Z","shell.execute_reply.started":"2024-01-11T06:01:39.703240Z","shell.execute_reply":"2024-01-11T06:13:52.693513Z"},"trusted":true},"execution_count":null,"outputs":[]}]}