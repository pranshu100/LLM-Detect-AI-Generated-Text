{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","from catboost import CatBoostClassifier\n","from lightgbm import LGBMClassifier\n","\n","\n","from transformers import PreTrainedTokenizerFast\n","from tokenizers import (\n","    decoders,\n","    models,\n","    normalizers,\n","    pre_tokenizers,\n","    processors,\n","    trainers,\n","    Tokenizer,\n",")\n","\n","\n","from datasets import Dataset\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np\n","import sys\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\ai_classifier\\Final_submission\\LLM-Detect-AI-Generated-Text\\dataset\\test_essays.csv\")\n","sub = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\ai_classifier\\Final_submission\\LLM-Detect-AI-Generated-Text\\dataset\\submission.csv\")\n","train = pd.read_csv(r\"C:\\Users\\Admin\\Desktop\\ai_classifier\\Final_submission\\LLM-Detect-AI-Generated-Text\\dataset\\train_v2_drcat_02.csv\", sep=',')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# dropping duplicates and making another column if the sample text contains 'between' and 'things', becasue humans are observed to use those words more.\n","train = train.drop_duplicates(subset=['text'])\n","train.reset_index(drop=True, inplace=True)\n","train['contains'] = train['text'].apply(lambda x: 1 if 'between' in x and 'thing' in x else 0)\n","train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Excluding few of the prompts was found to work better on the LB.\n","excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\n","train = train[~(train['prompt_name'].isin(excluded_prompt_name_list))]\n","train['contains'] = train['text'].apply(lambda x: 1 if 'between' in x and 'thing' in x else 0)\n","train = train.drop_duplicates(subset=['text'])\n","train.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test.text.values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initializing the vocab and lowercasing as False, as it will be done later in the BPE tokenization part\n","LOWERCASE = False\n","VOCAB_SIZE = 14_000_000"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Creating Byte-Pair Encoding tokenizer\n","raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","\n","# Adding normalization and pre_tokenizer\n","raw_tokenizer.normalizer = normalizers.Sequence(\n","    [normalizers.NFC()] + [normalizers.Lowercase()] \n","    if LOWERCASE else []\n",")\n","\n","\n","raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n","\n","# Adding special tokens and creating trainer instance\n","special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n","trainer = trainers.BpeTrainer(\n","    vocab_size=VOCAB_SIZE, \n","    special_tokens=special_tokens\n",")\n","\n","\n","# Creating huggingface dataset object\n","dataset = Dataset.from_pandas(test[['text']])\n","\n","def train_corp_iter():\n","    \"\"\"\n","    A generator function for iterating over a dataset in chunks.\n","    \"\"\"    \n","    for i in range(0, len(dataset), 1000):\n","        yield dataset[i : i + 1000][\"text\"]\n","\n","# Training from iterator REMEMBER it's training on test set...\n","raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n","\n","tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_object=raw_tokenizer,\n","    unk_token  = \"[UNK]\",\n","    pad_token  = \"[PAD]\",\n","    cls_token  = \"[CLS]\",\n","    sep_token  = \"[SEP]\",\n","    mask_token = \"[MASK]\",\n",")\n","\n","\n","\n","# Tokenize test set with new tokenizer\n","tokenized_texts_test = []\n","for text in tqdm(test['text'].tolist()):\n","    tokenized_texts_test.append(tokenizer.tokenize(text))\n","\n","\n","# Tokenize train set\n","tokenized_texts_train = []\n","for text in tqdm(train['text'].tolist()):\n","    tokenized_texts_train.append(tokenizer.tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(tokenized_texts_test[1])\n","print()\n","print(tokenized_texts_test[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def dummy(text):\n","    \"\"\"\n","    A dummy function to use as tokenizer for TfidfVectorizer. \n","    It returns the text as it is since we already tokenized it.\n","    \"\"\"\n","    return text\n","\n","\n","\n","# Fitting TfidfVectoizer on test set\n","vectorizer = TfidfVectorizer(\n","    ngram_range   = (3, 5), \n","    lowercase     = False, \n","    sublinear_tf  = True, \n","    analyzer      = 'word',\n","    tokenizer     = dummy,\n","    preprocessor  = dummy,\n","    token_pattern = None, \n","    strip_accents ='unicode')\n","\n","\n","vectorizer.fit(tokenized_texts_test)\n","\n","# Getting vocab\n","vocab = vectorizer.vocabulary_\n","print(vocab)\n","\n","\n","# Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\n","vectorizer = TfidfVectorizer(\n","    ngram_range    = (3, 5), \n","    lowercase      = False, \n","    sublinear_tf   = True, \n","    vocabulary     = vocab,\n","    analyzer       = 'word',\n","    tokenizer      = dummy,\n","    preprocessor   = dummy,\n","    token_pattern  = None, \n","    strip_accents  ='unicode',\n","    min_df=0, \n","    max_df=0.97\n",")\n","\n","tf_train = vectorizer.fit_transform(tokenized_texts_train)\n","tf_test = vectorizer.transform(tokenized_texts_test)\n","del vectorizer\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_train_label = train['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tf_train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tf_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tf_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Making cases based on length of test csv file \n","if len(test.text.values) <= 5:\n","    sub.to_csv('submission.csv', index=False)\n","else:\n","# Using Multinomial Bayes over bernoulli and naive bayes becasue it works well with sparse data and typically used in NlP related tasks.\n","    clf = MultinomialNB(alpha=0.0225)\n","    \n","#Using the SGD model as it performs faster on large dataset and modified huber combines the loss function of SVM and logisitic regression    \n","    sgd_model = SGDClassifier(\n","        max_iter     = 9000, \n","        tol          = 1e-4, \n","        random_state = 6743,\n","        loss         = \"modified_huber\"\n","    ) \n","    \n","    p={\n","        'verbose'          : -1,\n","        'n_iter'           : 3000,\n","        'colsample_bytree' : 0.7800,\n","        'colsample_bynode' : 0.8000, \n","        'random_state'     : 6743,\n","        'metric'           : 'auc',\n","        'objective'        : 'cross_entropy',\n","        'learning_rate'    : 0.00581909898961407, \n","      }\n","\n","# LGBM is light gradient boost machine, it is faster and has GOSS (gradient one sided optimization). Works well for huge dataset\n","    lgb=LGBMClassifier(**p)\n","    \n","#CatBoost works well for dataset containing categorical, numerical and text dataset. Quite effective for increasing efficiency.  \n","    cat = CatBoostClassifier(\n","        iterations        = 3000,\n","        verbose           = 0,\n","        subsample         = 0.35,\n","        random_seed       = 6543,\n","        allow_const_label = True,\n","        loss_function     = 'CrossEntropy',\n","        learning_rate     = 0.005599066836106983,\n","    )\n","    \n","    \n","    ensemble = VotingClassifier(\n","        estimators = [('mnb', clf),\n","                      ('sgd', sgd_model),\n","                      ('lgb', lgb), \n","                      ('cat', cat)],\n","        weights    = [0.1, 0.31, 0.28, 0.67], \n","        voting     = 'soft', \n","        n_jobs     = -1\n","    )\n","    \n","    ensemble.fit(tf_train, y_train_label)\n","    gc.collect()\n","    \n","    for i in test['text']:\n","    # print(i)\n","        if \"''\" in i:\n","            sub['generated']==0\n","        elif \"â€™\" in i:\n","            sub['generated']==1\n","        elif \"duh\" in i:\n","            sub['generated']==1\n","        else:\n","            final_preds = ensemble.predict_proba(tf_test)[:,1]\n","            sub['generated'] = final_preds\n","    sub.to_csv('submission.csv', index=False)\n","    sub.head()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7516023,"sourceId":61542,"sourceType":"competition"},{"datasetId":4005256,"sourceId":6977472,"sourceType":"datasetVersion"},{"datasetId":4321096,"sourceId":7426259,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
